{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primal hard SVM problem**\n",
    "We first consider hard SVM, that is, we want to find a hyperlane that seperates all datas. Let me start with the conclusion, then derive the idea behind it. The goal is to solve the following optimizatoin problem\n",
    "\n",
    "$$\\min_{W,b} \\frac{1}{2}W^T\\cdot W $$\n",
    "$$ \\mbox{s.t.  } y_n (W^Tx+b) \\ge 1 $$\n",
    "\n",
    "\n",
    "\n",
    "Since the idea of support vector machine is to maximize the distance of points to the separating hyper plane, we fist consider how to compute distance between a point and a plane.\n",
    "Let $<x, 1>$ be a fixed point in $\\mathbb{R}^{n+1}$ and a hyperplane $H:wx' + b = 0$in $\\mathbb{R}^{n+1}$. To compute thedistance between $x$ and $H$, note that the normal vector is $<w, 0>$. So the distance is \n",
    "\n",
    "$$d(x,b,W) = \\mid\\frac{W^T}{|W|}\\cdot (x-x')\\mid = \\frac{1}{|W|} \\mid W^Tx +b \\mid$$\n",
    "\n",
    "for any point $x'$ on $H$.\n",
    "\n",
    "Then the optimization problem becomes \n",
    "\n",
    "$$ \\max \\mbox{ margin}(b,W)$$\n",
    "$$  \\mbox{s.t. }  y_n(W^Tx_n+b) >0 , \\forall n $$\n",
    "\n",
    "where margin$(b,W)$ is $\\min_n \\frac{1}{|W|}y_n(W^Tx_n+b)$\n",
    "\n",
    "We can rescale $(W,b)$ without changing hyperplane, so we can consider the $(W,b)$\n",
    "such that $\\min_n y_n(W^Tx_n +b)= 1$.\n",
    "It is easy to see the margin happens at the data points where \n",
    "$y_n(W^Tx_n +b)= 1.$\n",
    "\n",
    "Then the optimization problem becomes\n",
    "\n",
    "$$ \\max \\frac{1}{|W|} $$\n",
    "$$  \\mbox{s.t. }  y_n(W^Tx_n+b) \\ge 1 , \\forall n $$\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "$$ \\min \\frac{1}{2}W^TW $$\n",
    "$$ \\mbox{s.t. }  y_n(W^Tx_n+b) \\ge 1 , \\forall 1\\le n \\le N $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dual SVM problem**\n",
    "For the primal SVM problem, we are solving $d+1$ variables $(W, b)$ with $N$ constrants. If $d$ is large, then hard to compute even using quadratic programming software. So we take the dual problem, the problem will be a quadratic optimizatoin probelm with $N$ varaibles and\n",
    "$N+1$ constraints. The good thing is the $N\\times N$ matrix will be sparse.\n",
    "\n",
    "Now we derive the dual problem. Consider the Lagrange problem\n",
    "$$\\mathcal{L}(W,b,\\alpha_n)= \\frac{1}{2}W^TW + \\sum_{n=1}^N \\alpha _n(1-y_n(W^Tx_n+b))$$\n",
    "We claim that the primal SVM problem is equivalent to \n",
    "$$\\min _{W,b} \\max_{\\alpha _n \\ge 0} \\mathcal{L}(W,b,\\alpha _n) $$\n",
    "\n",
    "First of all note that \n",
    "$$ \\max_{\\alpha _n} \\mathcal{L}(W,b,\\alpha _n) $$\n",
    "implies that the solution must be able to classiy all the datas.\n",
    "If not, there is some $i$ such that \n",
    "$$ y_n(W^Tx_n+b)< 1.$$\n",
    "The above equation will be $\\infty$, which is not possible to be $min$.\n",
    "\n",
    "Moreover, to get $max$, the $\\alpha _n $ must be $0$ if $ y_n(W^Tx_n+b)> 1.$\n",
    "\n",
    "W then have the following weak duality, where the left is\n",
    "equivalent to the primal problem and the right is the $dual$  $problem$,\n",
    "$$\\min _{W,b} \\max_{\\alpha _n \\ge 0} \\mathcal{L}(W,b,\\alpha _n) \n",
    "\\ge  \\max_{\\alpha _n \\ge 0} \\min _{W,b}\\mathcal{L}(W,b,\\alpha _n) $$\n",
    "\n",
    "**When does the weak duality become strong dualtiy?** i.e., the above inequality becomes equality.\n",
    "\n",
    "Now consider \n",
    "$$ \\min _{W,b}\\mathcal{L}(W,b,\\alpha _n)$$\n",
    "Take partial of $\\mathcal{L}$ with $b$ we get\n",
    "$$\\sum _{n= 1}^{N}\\alpha _n\\cdot y_n= 0 .$$\n",
    "Take partial of $\\mathcal{L}$ with respect to $W$ we get\n",
    "$$W_i = \\sum _{n= 1}^{N} \\alpha _n y_n x_{n, i}$$, or in vector form\n",
    "$$W = \\sum _{n= 1}^{N}\\alpha _n y_n X_n $$\n",
    "\n",
    "The dual problem becomes\n",
    "$$\\max _{\\alpha _n\\ge 0,\\sum \\alpha _n y_n= 0 , W = \\sum \\alpha _n y_n X_n }  \\frac{1}{2}W^TW + \\sum \\alpha _n - W^TW$$\n",
    "$$= \\max _{\\alpha _n\\ge 0,\\sum \\alpha _n y_n= 0 , W = \\sum \\alpha _n y_n X_n } -\\frac{1}{2} || \\sum \\alpha _n y_n X_n||^2 +\\sum\\alpha _n$$\n",
    "\n",
    "Here is the final form of the dual hard SVM problem\n",
    "$$ \\min \\frac{1}{2} \\sum_n \\sum_m \\alpha_n\\alpha_my_ny_mX^T_nX_m-\\sum \\alpha _n$$\n",
    "$$\\mbox{s.t. } \\alpha_n \\ge 0, \\sum \\alpha_n y_n = 0$$\n",
    "Solving for $N$ varaiables ($\\alpha_n$) with $N+1$ constraints.\n",
    "\n",
    "We then compute $b$ by \n",
    "$$1- y_n(W^TX_n+b) = 0$$ for $n$\n",
    "such that $\\alpha _n > 0, $ i.e. the support vector.\n",
    "\n",
    "**Note** I guess it is important to note that the optimal choice of \n",
    "$W$ is represented by  data, and we only consider the data point on the boundary. More precisely, \n",
    "$$ W_{SVM} = \\sum \\alpha_ny_nX_n .$$\n",
    "We also note that most of the $\\alpha_n$ are zero. This is one of the reasons that dual problem is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hard SVM with kernel trick**\n",
    "Translate the hard SVM problem into the form of CVXOPT package required.\n",
    "$$\\min \\frac{1}{2}\\alpha ^T P\\alpha -\\textbf{1}^T\\alpha$$\n",
    "$$\\mbox{s.t. } \\alpha \\ge 0, \\alpha ^T Y = 0$$\n",
    "were $P_{n,m} = y_ny_mX_n^TX_m $. More generally, we can make a transformation of $X$ first. That is \n",
    "$$P_{n,m} = y_ny_m \\Phi(X)^T \\Phi(X).$$\n",
    "We will try t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.0780e+02 -7.6366e+02  9e+02  4e-17  4e+01\n",
      " 1:  9.3245e+01  9.7637e+00  8e+01  8e-17  3e+00\n",
      " 2:  6.7311e+01  3.2553e+01  3e+01  5e-17  1e+00\n",
      " 3:  2.6071e+01  1.5068e+01  1e+01  9e-17  7e-01\n",
      " 4:  3.7092e+01  2.3152e+01  1e+01  2e-16  4e-01\n",
      " 5:  2.5352e+01  1.8652e+01  7e+00  3e-17  3e-16\n",
      " 6:  2.0062e+01  1.9974e+01  9e-02  5e-17  3e-16\n",
      " 7:  2.0001e+01  2.0000e+01  9e-04  1e-16  1e-16\n",
      " 8:  2.0000e+01  2.0000e+01  9e-06  2e-16  4e-16\n",
      "Optimal solution found.\n",
      "[ 7.13e-07]\n",
      "[ 5.00e+00]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we need to use the optimization package CVXOPT\n",
    "#To install : in terminal run 'conda install cvxopt'\n",
    "\n",
    "import numpy as np\n",
    "from cvxopt import matrix\n",
    "from cvxopt import solvers\n",
    "P = matrix([[1.0,0.0],[0.0,0.0]])\n",
    "q = matrix([3.0,4.0])\n",
    "G = matrix([[-1.0,0.0,-1.0,2.0,3.0],[0.0,-1.0,-3.0,5.0,4.0]])\n",
    "h = matrix([0.0,0.0,-15.0,100.0,80.0])\n",
    "sol = solvers.qp(P,q,G,h)\n",
    "print sol['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "a = [1,2,3]\n",
    "[f(x) for x in a]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
