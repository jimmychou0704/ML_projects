{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We first consider hard SVM, that is, we want to find a hyperlane that seperates all datas. Let me start with the conclusion, then derive the idea behind it. The goal is to solve the following optimizatoin problem\n",
    "\n",
    "$$\\min_{W,b} \\frac{1}{2}W^T\\cdot W $$\n",
    "$$ \\mbox{s.t.  } y_n (W^Tx+b) \\ge 1 $$\n",
    "\n",
    "\n",
    "\n",
    "Since the idea of support vector machine is to maximize the distance of points to the separating hyper plane, we fist consider how to compute distance between a point and a plane.\n",
    "Let $<x, 1>$ be a fixed point in $\\mathbb{R}^{n+1}$ and a hyperplane $H:wx' + b = 0$in $\\mathbb{R}^{n+1}$. To compute thedistance between $x$ and $H$, note that the normal vector is $<w, 0>$. So the distance is \n",
    "\n",
    "$$d(x,b,W) = \\mid\\frac{W^T}{|W|}\\cdot (x-x')\\mid = \\frac{1}{|W|} \\mid W^Tx +b \\mid$$\n",
    "\n",
    "for any point $x'$ on $H$.\n",
    "\n",
    "Then the optimization problem becomes \n",
    "\n",
    "$$ \\max \\mbox{ margin}(b,W)$$\n",
    "$$  \\mbox{s.t. }  y_n(W^Tx_n+b) >0 , \\forall n $$\n",
    "\n",
    "where margin$(b,W)$ is $\\min_n \\frac{1}{|W|}y_n(W^Tx_n+b)$\n",
    "\n",
    "We can rescale $(W,b)$ without changing hyperplane, so we can consider the $(W,b)$\n",
    "such that $\\min_n y_n(W^Tx_n +b)= 1$.\n",
    "It is easy to see the margin happens at the data points where \n",
    "$y_n(W^Tx_n +b)= 1.$\n",
    "\n",
    "Then the optimization problem becomes\n",
    "\n",
    "$$ \\max \\frac{1}{|W|} $$\n",
    "$$  \\mbox{s.t. }  y_n(W^Tx_n+b) \\ge 1 , \\forall n $$\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "$$ \\min \\frac{1}{2}W^TW $$\n",
    "$$ \\mbox{s.t. }  y_n(W^Tx_n+b) \\ge 1 , \\forall 1\\le n \\le N $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dual SVM problem**\n",
    "For the primal SVM problem, we are solving $d+1$ variables $(W, b)$ with $N$ constrants. If $d$ is large, then hard to compute even using quadratic programming software. So we take the dual problem, the problem will be a quadratic optimizatoin probelm with $N$ varaibles and\n",
    "$N+1$ constraints. The good thing is the $N\\times N$ matrix will be sparse.\n",
    "\n",
    "Now we derive the dual problem. Consider the Lagrange problem\n",
    "$$\\mathcal{L}(W,b,\\alpha_n)= \\frac{1}{2}W^TW + \\sum_{n=1}^N \\alpha _n(1-y_n(W^Tx_n+b))$$\n",
    "We claim that the primal SVM problem is equivalent to \n",
    "$$\\min _{W,b} \\max_{\\alpha _n} \\mathcal{L}(W,b,\\alpha _n) $$\n",
    "\n",
    "First of all note that \n",
    "$$ \\max_{\\alpha _n} \\mathcal{L}(W,b,\\alpha _n) $$\n",
    "implies that the solution must be able to classiy all the data, otherwise the above equation will be $\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
